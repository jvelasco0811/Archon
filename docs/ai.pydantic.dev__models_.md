[ Skip to content ](https://ai.pydantic.dev/models/#models-interfaces-and-providers)
[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/ "PydanticAI")
PydanticAI 
Models 
Type to start searching
[ pydantic/pydantic-ai  ](https://github.com/pydantic/pydantic-ai "Go to repository")
[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/ "PydanticAI") PydanticAI 
[ pydantic/pydantic-ai  ](https://github.com/pydantic/pydantic-ai "Go to repository")
  * [ Introduction  ](https://ai.pydantic.dev/)
  * [ Installation  ](https://ai.pydantic.dev/install/)
  * [ Getting Help  ](https://ai.pydantic.dev/help/)
  * [ Contributing  ](https://ai.pydantic.dev/contributing/)
  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)
  * Documentation  Documentation 
    * [ Agents  ](https://ai.pydantic.dev/agents/)
    * Models  [ Models  ](https://ai.pydantic.dev/models/) Table of contents 
      * [ Models, Interfaces, and Providers  ](https://ai.pydantic.dev/models/#models-interfaces-and-providers)
      * [ OpenAI  ](https://ai.pydantic.dev/models/#openai)
        * [ Install  ](https://ai.pydantic.dev/models/#install)
        * [ Configuration  ](https://ai.pydantic.dev/models/#configuration)
        * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable)
        * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument)
        * [ Custom OpenAI Client  ](https://ai.pydantic.dev/models/#custom-openai-client)
      * [ Anthropic  ](https://ai.pydantic.dev/models/#anthropic)
        * [ Install  ](https://ai.pydantic.dev/models/#install_1)
        * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_1)
        * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable_1)
        * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument_1)
        * [ Custom HTTP Client  ](https://ai.pydantic.dev/models/#custom-http-client)
      * [ Gemini  ](https://ai.pydantic.dev/models/#gemini)
        * [ Install  ](https://ai.pydantic.dev/models/#install_2)
        * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_2)
        * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable_2)
        * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument_2)
      * [ Gemini via VertexAI  ](https://ai.pydantic.dev/models/#gemini-via-vertexai)
        * [ Install  ](https://ai.pydantic.dev/models/#install_3)
        * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_3)
        * [ Application default credentials  ](https://ai.pydantic.dev/models/#application-default-credentials)
        * [ Service account  ](https://ai.pydantic.dev/models/#service-account)
        * [ Customising region  ](https://ai.pydantic.dev/models/#customising-region)
      * [ Groq  ](https://ai.pydantic.dev/models/#groq)
        * [ Install  ](https://ai.pydantic.dev/models/#install_4)
        * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_4)
        * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable_3)
        * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument_3)
      * [ Mistral  ](https://ai.pydantic.dev/models/#mistral)
        * [ Install  ](https://ai.pydantic.dev/models/#install_5)
        * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_5)
        * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable_4)
        * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument_4)
      * [ Cohere  ](https://ai.pydantic.dev/models/#cohere)
        * [ Install  ](https://ai.pydantic.dev/models/#install_6)
        * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_6)
        * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable_5)
        * [ api_key argument  ](https://ai.pydantic.dev/models/#api_key-argument)
      * [ Bedrock  ](https://ai.pydantic.dev/models/#bedrock)
        * [ Install  ](https://ai.pydantic.dev/models/#install_7)
        * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_7)
        * [ Environment variables  ](https://ai.pydantic.dev/models/#environment-variables)
        * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument_5)
      * [ OpenAI-compatible Models  ](https://ai.pydantic.dev/models/#openai-compatible-models)
        * [ Ollama  ](https://ai.pydantic.dev/models/#ollama)
          * [ Example local usage  ](https://ai.pydantic.dev/models/#example-local-usage)
          * [ Example using a remote server  ](https://ai.pydantic.dev/models/#example-using-a-remote-server)
        * [ Azure AI Foundry  ](https://ai.pydantic.dev/models/#azure-ai-foundry)
        * [ OpenRouter  ](https://ai.pydantic.dev/models/#openrouter)
        * [ Grok (xAI)  ](https://ai.pydantic.dev/models/#grok-xai)
        * [ Perplexity  ](https://ai.pydantic.dev/models/#perplexity)
        * [ Fireworks AI  ](https://ai.pydantic.dev/models/#fireworks-ai)
        * [ Together AI  ](https://ai.pydantic.dev/models/#together-ai)
      * [ Implementing Custom Models  ](https://ai.pydantic.dev/models/#implementing-custom-models)
      * [ Fallback  ](https://ai.pydantic.dev/models/#fallback)
    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)
    * [ Function Tools  ](https://ai.pydantic.dev/tools/)
    * [ Common Tools  ](https://ai.pydantic.dev/common_tools/)
    * [ Results  ](https://ai.pydantic.dev/results/)
    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)
    * [ Testing and Evals  ](https://ai.pydantic.dev/testing-evals/)
    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)
    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)
    * [ Graphs  ](https://ai.pydantic.dev/graph/)
    * [ Image, Audio & Document Input  ](https://ai.pydantic.dev/input/)
    * [ MCP  ](https://ai.pydantic.dev/mcp/)
MCP 
      * [ Client  ](https://ai.pydantic.dev/mcp/client/)
      * [ Server  ](https://ai.pydantic.dev/mcp/server/)
      * [ MCP Run Python  ](https://ai.pydantic.dev/mcp/run-python/)
    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)
  * [ Examples  ](https://ai.pydantic.dev/examples/)
Examples 
    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)
    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)
    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)
    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)
    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)
    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)
    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)
    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)
    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)
    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)
  * API Reference  API Reference 
    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)
    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)
    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)
    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)
    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)
    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)
    * [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/)
    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)
    * [ pydantic_ai.mcp  ](https://ai.pydantic.dev/api/mcp/)
    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)
    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)
    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)
    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)
    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)
    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)
    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)
    * [ pydantic_ai.models.vertexai  ](https://ai.pydantic.dev/api/models/vertexai/)
    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)
    * [ pydantic_ai.models.instrumented  ](https://ai.pydantic.dev/api/models/instrumented/)
    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)
    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)
    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)
    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)
    * [ pydantic_ai.models.wrapper  ](https://ai.pydantic.dev/api/models/wrapper/)
    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)
    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)
    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)
    * [ pydantic_graph.persistence  ](https://ai.pydantic.dev/api/pydantic_graph/persistence/)
    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)
    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)


Table of contents 
  * [ Models, Interfaces, and Providers  ](https://ai.pydantic.dev/models/#models-interfaces-and-providers)
  * [ OpenAI  ](https://ai.pydantic.dev/models/#openai)
    * [ Install  ](https://ai.pydantic.dev/models/#install)
    * [ Configuration  ](https://ai.pydantic.dev/models/#configuration)
    * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable)
    * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument)
    * [ Custom OpenAI Client  ](https://ai.pydantic.dev/models/#custom-openai-client)
  * [ Anthropic  ](https://ai.pydantic.dev/models/#anthropic)
    * [ Install  ](https://ai.pydantic.dev/models/#install_1)
    * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_1)
    * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable_1)
    * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument_1)
    * [ Custom HTTP Client  ](https://ai.pydantic.dev/models/#custom-http-client)
  * [ Gemini  ](https://ai.pydantic.dev/models/#gemini)
    * [ Install  ](https://ai.pydantic.dev/models/#install_2)
    * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_2)
    * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable_2)
    * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument_2)
  * [ Gemini via VertexAI  ](https://ai.pydantic.dev/models/#gemini-via-vertexai)
    * [ Install  ](https://ai.pydantic.dev/models/#install_3)
    * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_3)
    * [ Application default credentials  ](https://ai.pydantic.dev/models/#application-default-credentials)
    * [ Service account  ](https://ai.pydantic.dev/models/#service-account)
    * [ Customising region  ](https://ai.pydantic.dev/models/#customising-region)
  * [ Groq  ](https://ai.pydantic.dev/models/#groq)
    * [ Install  ](https://ai.pydantic.dev/models/#install_4)
    * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_4)
    * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable_3)
    * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument_3)
  * [ Mistral  ](https://ai.pydantic.dev/models/#mistral)
    * [ Install  ](https://ai.pydantic.dev/models/#install_5)
    * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_5)
    * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable_4)
    * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument_4)
  * [ Cohere  ](https://ai.pydantic.dev/models/#cohere)
    * [ Install  ](https://ai.pydantic.dev/models/#install_6)
    * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_6)
    * [ Environment variable  ](https://ai.pydantic.dev/models/#environment-variable_5)
    * [ api_key argument  ](https://ai.pydantic.dev/models/#api_key-argument)
  * [ Bedrock  ](https://ai.pydantic.dev/models/#bedrock)
    * [ Install  ](https://ai.pydantic.dev/models/#install_7)
    * [ Configuration  ](https://ai.pydantic.dev/models/#configuration_7)
    * [ Environment variables  ](https://ai.pydantic.dev/models/#environment-variables)
    * [ provider argument  ](https://ai.pydantic.dev/models/#provider-argument_5)
  * [ OpenAI-compatible Models  ](https://ai.pydantic.dev/models/#openai-compatible-models)
    * [ Ollama  ](https://ai.pydantic.dev/models/#ollama)
      * [ Example local usage  ](https://ai.pydantic.dev/models/#example-local-usage)
      * [ Example using a remote server  ](https://ai.pydantic.dev/models/#example-using-a-remote-server)
    * [ Azure AI Foundry  ](https://ai.pydantic.dev/models/#azure-ai-foundry)
    * [ OpenRouter  ](https://ai.pydantic.dev/models/#openrouter)
    * [ Grok (xAI)  ](https://ai.pydantic.dev/models/#grok-xai)
    * [ Perplexity  ](https://ai.pydantic.dev/models/#perplexity)
    * [ Fireworks AI  ](https://ai.pydantic.dev/models/#fireworks-ai)
    * [ Together AI  ](https://ai.pydantic.dev/models/#together-ai)
  * [ Implementing Custom Models  ](https://ai.pydantic.dev/models/#implementing-custom-models)
  * [ Fallback  ](https://ai.pydantic.dev/models/#fallback)


# Models
PydanticAI is Model-agnostic and has built in support for the following model providers:
  * [OpenAI](https://ai.pydantic.dev/models/#openai)
  * [Anthropic](https://ai.pydantic.dev/models/#anthropic)
  * Gemini via two different APIs: [Generative Language API](https://ai.pydantic.dev/models/#gemini) and [VertexAI API](https://ai.pydantic.dev/models/#gemini-via-vertexai)
  * [Ollama](https://ai.pydantic.dev/models/#ollama)
  * [Groq](https://ai.pydantic.dev/models/#groq)
  * [Mistral](https://ai.pydantic.dev/models/#mistral)
  * [Cohere](https://ai.pydantic.dev/models/#cohere)
  * [Bedrock](https://ai.pydantic.dev/models/#bedrock)


See [OpenAI-compatible models](https://ai.pydantic.dev/models/#openai-compatible-models) for more examples on how to use models such as [OpenRouter](https://ai.pydantic.dev/models/#openrouter), and [Grok (xAI)](https://ai.pydantic.dev/models/#grok-xai) that support the OpenAI SDK.
You can also [add support for other models](https://ai.pydantic.dev/models/#implementing-custom-models).
PydanticAI also comes with [`TestModel`](https://ai.pydantic.dev/api/models/test/) and [`FunctionModel`](https://ai.pydantic.dev/api/models/function/) for testing and development.
To use each model provider, you need to configure your local environment and make sure you have the right packages installed.
## Models, Interfaces, and Providers
PydanticAI uses a few key terms to describe how it interacts with different LLMs:
  * **Model** : This refers to the specific LLM model you want to handle your requests (e.g., `gpt-4o`, `claude-3-5-sonnet-latest`, `gemini-1.5-flash`). It's the "brain" that processes your prompts and generates responses. You specify the _Model_ as a parameter to the _Interface_.
  * **Interface** : This refers to a PydanticAI class used to make requests following a specific LLM API (generally by wrapping a vendor-provided SDK, like the `openai` python SDK). These classes implement a vendor-SDK-agnostic API, ensuring a single PydanticAI agent is portable to different LLM vendors without any other code changes just by swapping out the _Interface_ it uses. Currently, interface classes are named roughly in the format `<VendorSdk>Model`, for example, we have `OpenAIModel`, `AnthropicModel`, `GeminiModel`, etc. These `Model` classes will soon be renamed to `<VendorSdk>Interface` to reflect this terminology better.
  * **Provider** : This refers to _Interface_ -specific classes which handle the authentication and connections to an LLM vendor. Passing a non-default _Provider_ as a parameter to an _Interface_ is how you can ensure that your agent will make requests to a specific endpoint, or make use of a specific approach to authentication (e.g., you can use Vertex-specific auth with the `GeminiModel` by way of the `VertexProvider`). In particular, this is how you can make use of an AI gateway, or an LLM vendor that offers API compatibility with the vendor SDK used by an existing interface (such as `OpenAIModel`).


In short, you select a _model_ , PydanticAI uses the appropriate _interface_ class, and the _provider_ handles the connection and authentication to the underlying service.
## OpenAI
### Install
To use OpenAI models, you need to either install [`pydantic-ai`](https://ai.pydantic.dev/install/), or install [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) with the `openai` optional group:
[pip](https://ai.pydantic.dev/models/#__tabbed_1_1)[uv](https://ai.pydantic.dev/models/#__tabbed_1_2)
```
pipinstall"pydantic-ai-slim[openai]"

```

```
uvadd"pydantic-ai-slim[openai]"

```

### Configuration
To use [`OpenAIModel`](https://ai.pydantic.dev/api/models/openai/#pydantic_ai.models.openai.OpenAIModel) through their main API, go to [platform.openai.com](https://platform.openai.com/) and follow your nose until you find the place to generate an API key.
### Environment variable
Once you have the API key, you can set it as an environment variable:
```
exportOPENAI_API_KEY='your-api-key'

```

You can then use [`OpenAIModel`](https://ai.pydantic.dev/api/models/openai/#pydantic_ai.models.openai.OpenAIModel) by name:
openai_model_by_name.py```
frompydantic_aiimport Agent
agent = Agent('openai:gpt-4o')
...

```

Or initialise the model directly with just the model name:
openai_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
model = OpenAIModel('gpt-4o')
agent = Agent(model)
...

```

By default, the `OpenAIModel` uses the [`OpenAIProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.openai.OpenAIProvider.__init__) with the `base_url` set to `https://api.openai.com/v1`. 
### `provider` argument
You can provide a custom [`Provider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.Provider) via the [`provider` argument](https://ai.pydantic.dev/api/models/openai/#pydantic_ai.models.openai.OpenAIModel.__init__):
openai_model_provider.py```
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.openaiimport OpenAIProvider
model = OpenAIModel('gpt-4o', provider=OpenAIProvider(api_key='your-api-key'))
agent = Agent(model)
...

```

### Custom OpenAI Client
`OpenAIProvider` also accepts a custom `AsyncOpenAI` client via the [`openai_client` parameter](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.openai.OpenAIProvider.__init__), so you can customise the `organization`, `project`, `base_url` etc. as defined in the [OpenAI API docs](https://platform.openai.com/docs/api-reference).
You could also use the [`AsyncAzureOpenAI`](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints) client to use the Azure OpenAI API.
openai_azure.py```
fromopenaiimport AsyncAzureOpenAI
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.openaiimport OpenAIProvider
client = AsyncAzureOpenAI(
  azure_endpoint='...',
  api_version='2024-07-01-preview',
  api_key='your-api-key',
)
model = OpenAIModel(
  'gpt-4o',
  provider=OpenAIProvider(openai_client=client),
)
agent = Agent(model)
...

```

## Anthropic
### Install
To use [`AnthropicModel`](https://ai.pydantic.dev/api/models/anthropic/#pydantic_ai.models.anthropic.AnthropicModel) models, you need to either install [`pydantic-ai`](https://ai.pydantic.dev/install/), or install [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) with the `anthropic` optional group:
[pip](https://ai.pydantic.dev/models/#__tabbed_2_1)[uv](https://ai.pydantic.dev/models/#__tabbed_2_2)
```
pipinstall"pydantic-ai-slim[anthropic]"

```

```
uvadd"pydantic-ai-slim[anthropic]"

```

### Configuration
To use [Anthropic](https://anthropic.com) through their API, go to [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys) to generate an API key.
[`AnthropicModelName`](https://ai.pydantic.dev/api/models/anthropic/#pydantic_ai.models.anthropic.AnthropicModelName) contains a list of available Anthropic models.
### Environment variable
Once you have the API key, you can set it as an environment variable:
```
exportANTHROPIC_API_KEY='your-api-key'

```

You can then use [`AnthropicModel`](https://ai.pydantic.dev/api/models/anthropic/#pydantic_ai.models.anthropic.AnthropicModel) by name:
anthropic_model_by_name.py```
frompydantic_aiimport Agent
agent = Agent('anthropic:claude-3-5-sonnet-latest')
...

```

Or initialise the model directly with just the model name:
anthropic_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.anthropicimport AnthropicModel
model = AnthropicModel('claude-3-5-sonnet-latest')
agent = Agent(model)
...

```

### `provider` argument
You can provide a custom [`Provider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.Provider) via the [`provider` argument](https://ai.pydantic.dev/api/models/anthropic/#pydantic_ai.models.anthropic.AnthropicModel.__init__):
anthropic_model_provider.py```
frompydantic_aiimport Agent
frompydantic_ai.models.anthropicimport AnthropicModel
frompydantic_ai.providers.anthropicimport AnthropicProvider
model = AnthropicModel(
  'claude-3-5-sonnet-latest', provider=AnthropicProvider(api_key='your-api-key')
)
agent = Agent(model)
...

```

### Custom HTTP Client
You can customize the `AnthropicProvider` with a custom `httpx.AsyncClient`:
anthropic_model_custom_provider.py```
fromhttpximport AsyncClient
frompydantic_aiimport Agent
frompydantic_ai.models.anthropicimport AnthropicModel
frompydantic_ai.providers.anthropicimport AnthropicProvider
custom_http_client = AsyncClient(timeout=30)
model = AnthropicModel(
  'claude-3-5-sonnet-latest',
  provider=AnthropicProvider(api_key='your-api-key', http_client=custom_http_client),
)
agent = Agent(model)
...

```

## Gemini
### Install
To use [`GeminiModel`](https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModel) models, you just need to install [`pydantic-ai`](https://ai.pydantic.dev/install/) or [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install), no extra dependencies are required.
### Configuration
[`GeminiModel`](https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModel) let's you use the Google's Gemini models through their [Generative Language API](https://ai.google.dev/api/all-methods), `generativelanguage.googleapis.com`.
[`GeminiModelName`](https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModelName) contains a list of available Gemini models that can be used through this interface.
To use `GeminiModel`, go to [aistudio.google.com](https://aistudio.google.com/apikey) and select "Create API key".
### Environment variable
Once you have the API key, you can set it as an environment variable:
```
exportGEMINI_API_KEY=your-api-key

```

You can then use [`GeminiModel`](https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModel) by name:
gemini_model_by_name.py```
frompydantic_aiimport Agent
agent = Agent('google-gla:gemini-2.0-flash')
...

```

Note
The `google-gla` provider prefix represents the [Google **G** enerative **L** anguage **A** PI](https://ai.google.dev/api/all-methods) for `GeminiModel`s. `google-vertex` is used with [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models).
Or initialise the model directly with just the model name and provider:
gemini_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.geminiimport GeminiModel
model = GeminiModel('gemini-2.0-flash', provider='google-gla')
agent = Agent(model)
...

```

### `provider` argument
You can provide a custom [`Provider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.Provider) via the [`provider` argument](https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModel.__init__):
gemini_model_provider.py```
frompydantic_aiimport Agent
frompydantic_ai.models.geminiimport GeminiModel
frompydantic_ai.providers.google_glaimport GoogleGLAProvider
model = GeminiModel(
  'gemini-2.0-flash', provider=GoogleGLAProvider(api_key='your-api-key')
)
agent = Agent(model)
...

```

You can also customize the `GoogleGLAProvider` with a custom `http_client`: 
gemini_model_custom_provider.py```
fromhttpximport AsyncClient
frompydantic_aiimport Agent
frompydantic_ai.models.geminiimport GeminiModel
frompydantic_ai.providers.google_glaimport GoogleGLAProvider
custom_http_client = AsyncClient(timeout=30)
model = GeminiModel(
  'gemini-2.0-flash',
  provider=GoogleGLAProvider(api_key='your-api-key', http_client=custom_http_client),
)
agent = Agent(model)
...

```

## Gemini via VertexAI
If you are an enterprise user, you should use the `google-vertex` provider with [`GeminiModel`](https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModel) which uses the `*-aiplatform.googleapis.com` API.
[`GeminiModelName`](https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModelName) contains a list of available Gemini models that can be used through this interface.
### Install
To use the `google-vertex` provider with [`GeminiModel`](https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModel), you need to either install [`pydantic-ai`](https://ai.pydantic.dev/install/), or install [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) with the `vertexai` optional group:
[pip](https://ai.pydantic.dev/models/#__tabbed_3_1)[uv](https://ai.pydantic.dev/models/#__tabbed_3_2)
```
pipinstall"pydantic-ai-slim[vertexai]"

```

```
uvadd"pydantic-ai-slim[vertexai]"

```

### Configuration
This interface has a number of advantages over `generativelanguage.googleapis.com` documented above:
  1. The VertexAI API comes with more enterprise readiness guarantees.
  2. You can [purchase provisioned throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput#purchase-provisioned-throughput) with VertexAI to guarantee capacity.
  3. If you're running PydanticAI inside GCP, you don't need to set up authentication, it should "just work".
  4. You can decide which region to use, which might be important from a regulatory perspective, and might improve latency.


The big disadvantage is that for local development you may need to create and configure a "service account", which I've found extremely painful to get right in the past.
Whichever way you authenticate, you'll need to have VertexAI enabled in your GCP account.
### Application default credentials
Luckily if you're running PydanticAI inside GCP, or you have the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud) installed and configured, you should be able to use `VertexAIModel` without any additional setup.
To use `VertexAIModel`, with [application default credentials](https://cloud.google.com/docs/authentication/application-default-credentials) configured (e.g. with `gcloud`), you can simply use:
vertexai_application_default_credentials.py```
frompydantic_aiimport Agent
frompydantic_ai.models.geminiimport GeminiModel
model = GeminiModel('gemini-2.0-flash', provider='google-vertex')
agent = Agent(model)
...

```

Internally this uses [`google.auth.default()`](https://google-auth.readthedocs.io/en/master/reference/google.auth.html) from the `google-auth` package to obtain credentials.
Won't fail until `agent.run()`
Because `google.auth.default()` requires network requests and can be slow, it's not run until you call `agent.run()`.
You may also need to pass the [`project_id` argument to `GoogleVertexProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.google_vertex.GoogleVertexProvider) if application default credentials don't set a project, if you pass `project_id` and it conflicts with the project set by application default credentials, an error is raised.
### Service account
If instead of application default credentials, you want to authenticate with a service account, you'll need to create a service account, add it to your GCP project (note: AFAIK this step is necessary even if you created the service account within the project), give that service account the "Vertex AI Service Agent" role, and download the service account JSON file.
Once you have the JSON file, you can use it thus:
vertexai_service_account.py```
frompydantic_aiimport Agent
frompydantic_ai.models.geminiimport GeminiModel
frompydantic_ai.providers.google_verteximport GoogleVertexProvider
model = GeminiModel(
  'gemini-2.0-flash',
  provider=GoogleVertexProvider(service_account_file='path/to/service-account.json'),
)
agent = Agent(model)
...

```

Alternatively, if you already have the service account information in memory, you can pass it as a dictionary:
vertexai_service_account.py```
importjson
frompydantic_aiimport Agent
frompydantic_ai.models.geminiimport GeminiModel
frompydantic_ai.providers.google_verteximport GoogleVertexProvider
service_account_info = json.loads(
  '{"type": "service_account", "project_id": "my-project-id"}'
)
model = GeminiModel(
  'gemini-2.0-flash',
  provider=GoogleVertexProvider(service_account_info=service_account_info),
)
agent = Agent(model)
...

```

### Customising region
Whichever way you authenticate, you can specify which region requests will be sent to via the [`region` argument](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.google_vertex.GoogleVertexProvider).
Using a region close to your application can improve latency and might be important from a regulatory perspective.
vertexai_region.py```
frompydantic_aiimport Agent
frompydantic_ai.models.geminiimport GeminiModel
frompydantic_ai.providers.google_verteximport GoogleVertexProvider
model = GeminiModel(
  'gemini-2.0-flash', provider=GoogleVertexProvider(region='asia-east1')
)
agent = Agent(model)
...

```

You can also customize the `GoogleVertexProvider` with a custom `http_client`: 
vertexai_custom_provider.py```
fromhttpximport AsyncClient
frompydantic_aiimport Agent
frompydantic_ai.models.geminiimport GeminiModel
frompydantic_ai.providers.google_verteximport GoogleVertexProvider
custom_http_client = AsyncClient(timeout=30)
model = GeminiModel(
  'gemini-2.0-flash',
  provider=GoogleVertexProvider(region='asia-east1', http_client=custom_http_client),
)
agent = Agent(model)
...

```

## Groq
### Install
To use [`GroqModel`](https://ai.pydantic.dev/api/models/groq/#pydantic_ai.models.groq.GroqModel), you need to either install [`pydantic-ai`](https://ai.pydantic.dev/install/), or install [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) with the `groq` optional group:
[pip](https://ai.pydantic.dev/models/#__tabbed_4_1)[uv](https://ai.pydantic.dev/models/#__tabbed_4_2)
```
pipinstall"pydantic-ai-slim[groq]"

```

```
uvadd"pydantic-ai-slim[groq]"

```

### Configuration
To use [Groq](https://groq.com/) through their API, go to [console.groq.com/keys](https://console.groq.com/keys) and follow your nose until you find the place to generate an API key.
[`GroqModelName`](https://ai.pydantic.dev/api/models/groq/#pydantic_ai.models.groq.GroqModelName) contains a list of available Groq models.
### Environment variable
Once you have the API key, you can set it as an environment variable:
```
exportGROQ_API_KEY='your-api-key'

```

You can then use [`GroqModel`](https://ai.pydantic.dev/api/models/groq/#pydantic_ai.models.groq.GroqModel) by name:
groq_model_by_name.py```
frompydantic_aiimport Agent
agent = Agent('groq:llama-3.3-70b-versatile')
...

```

Or initialise the model directly with just the model name:
groq_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.groqimport GroqModel
model = GroqModel('llama-3.3-70b-versatile')
agent = Agent(model)
...

```

### `provider` argument
You can provide a custom [`Provider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.Provider) via the [`provider` argument](https://ai.pydantic.dev/api/models/groq/#pydantic_ai.models.groq.GroqModel.__init__):
groq_model_provider.py```
frompydantic_aiimport Agent
frompydantic_ai.models.groqimport GroqModel
frompydantic_ai.providers.groqimport GroqProvider
model = GroqModel(
  'llama-3.3-70b-versatile', provider=GroqProvider(api_key='your-api-key')
)
agent = Agent(model)
...

```

You can also customize the [`GroqProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.groq.GroqProvider) with a custom `httpx.AsyncHTTPClient`:
groq_model_custom_provider.py```
fromhttpximport AsyncClient
frompydantic_aiimport Agent
frompydantic_ai.models.groqimport GroqModel
frompydantic_ai.providers.groqimport GroqProvider
custom_http_client = AsyncClient(timeout=30)
model = GroqModel(
  'llama-3.3-70b-versatile',
  provider=GroqProvider(api_key='your-api-key', http_client=custom_http_client),
)
agent = Agent(model)
...

```

## Mistral
### Install
To use [`MistralModel`](https://ai.pydantic.dev/api/models/mistral/#pydantic_ai.models.mistral.MistralModel), you need to either install [`pydantic-ai`](https://ai.pydantic.dev/install/), or install [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) with the `mistral` optional group:
[pip](https://ai.pydantic.dev/models/#__tabbed_5_1)[uv](https://ai.pydantic.dev/models/#__tabbed_5_2)
```
pipinstall"pydantic-ai-slim[mistral]"

```

```
uvadd"pydantic-ai-slim[mistral]"

```

### Configuration
To use [Mistral](https://mistral.ai) through their API, go to [console.mistral.ai/api-keys/](https://console.mistral.ai/api-keys/) and follow your nose until you find the place to generate an API key.
[`LatestMistralModelNames`](https://ai.pydantic.dev/api/models/mistral/#pydantic_ai.models.mistral.LatestMistralModelNames) contains a list of the most popular Mistral models.
### Environment variable
Once you have the API key, you can set it as an environment variable:
```
exportMISTRAL_API_KEY='your-api-key'

```

You can then use [`MistralModel`](https://ai.pydantic.dev/api/models/mistral/#pydantic_ai.models.mistral.MistralModel) by name:
mistral_model_by_name.py```
frompydantic_aiimport Agent
agent = Agent('mistral:mistral-large-latest')
...

```

Or initialise the model directly with just the model name:
mistral_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.mistralimport MistralModel
model = MistralModel('mistral-small-latest')
agent = Agent(model)
...

```

### `provider` argument
You can provide a custom [`Provider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.Provider) via the [`provider` argument](https://ai.pydantic.dev/api/models/mistral/#pydantic_ai.models.mistral.MistralModel.__init__):
groq_model_provider.py```
frompydantic_aiimport Agent
frompydantic_ai.models.mistralimport MistralModel
frompydantic_ai.providers.mistralimport MistralProvider
model = MistralModel(
  'mistral-large-latest', provider=MistralProvider(api_key='your-api-key')
)
agent = Agent(model)
...

```

You can also customize the provider with a custom `httpx.AsyncHTTPClient`:
groq_model_custom_provider.py```
fromhttpximport AsyncClient
frompydantic_aiimport Agent
frompydantic_ai.models.mistralimport MistralModel
frompydantic_ai.providers.mistralimport MistralProvider
custom_http_client = AsyncClient(timeout=30)
model = MistralModel(
  'mistral-large-latest',
  provider=MistralProvider(api_key='your-api-key', http_client=custom_http_client),
)
agent = Agent(model)
...

```

## Cohere
### Install
To use [`CohereModel`](https://ai.pydantic.dev/api/models/cohere/#pydantic_ai.models.cohere.CohereModel), you need to either install [`pydantic-ai`](https://ai.pydantic.dev/install/), or install [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) with the `cohere` optional group:
[pip](https://ai.pydantic.dev/models/#__tabbed_6_1)[uv](https://ai.pydantic.dev/models/#__tabbed_6_2)
```
pipinstall"pydantic-ai-slim[cohere]"

```

```
uvadd"pydantic-ai-slim[cohere]"

```

### Configuration
To use [Cohere](https://cohere.com/) through their API, go to [dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys) and follow your nose until you find the place to generate an API key.
[`CohereModelName`](https://ai.pydantic.dev/api/models/cohere/#pydantic_ai.models.cohere.CohereModelName) contains a list of the most popular Cohere models.
### Environment variable
Once you have the API key, you can set it as an environment variable:
```
exportCO_API_KEY='your-api-key'

```

You can then use [`CohereModel`](https://ai.pydantic.dev/api/models/cohere/#pydantic_ai.models.cohere.CohereModel) by name:
cohere_model_by_name.py```
frompydantic_aiimport Agent
agent = Agent('cohere:command')
...

```

Or initialise the model directly with just the model name:
cohere_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.cohereimport CohereModel
model = CohereModel('command', api_key='your-api-key')
agent = Agent(model)
...

```

### `api_key` argument
If you don't want to or can't set the environment variable, you can pass it at runtime via the [`api_key` argument](https://ai.pydantic.dev/api/models/cohere/#pydantic_ai.models.cohere.CohereModel.__init__):
cohere_model_api_key.py```
frompydantic_aiimport Agent
frompydantic_ai.models.cohereimport CohereModel
model = CohereModel('command', api_key='your-api-key')
agent = Agent(model)
...

```

## Bedrock
### Install
To use [`BedrockConverseModel`](https://ai.pydantic.dev/api/models/bedrock/#pydantic_ai.models.bedrock.BedrockConverseModel), you need to either install [`pydantic-ai`](https://ai.pydantic.dev/install/), or install [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) with the `bedrock` optional group:
[pip](https://ai.pydantic.dev/models/#__tabbed_7_1)[uv](https://ai.pydantic.dev/models/#__tabbed_7_2)
```
pipinstall"pydantic-ai-slim[bedrock]"

```

```
uvadd"pydantic-ai-slim[bedrock]"

```

### Configuration
To use [AWS Bedrock](https://aws.amazon.com/bedrock/), you'll need an AWS account with Bedrock enabled and appropriate credentials. You can use either AWS credentials directly or a pre-configured boto3 client.
[`BedrockModelName`](https://ai.pydantic.dev/api/models/bedrock/#pydantic_ai.models.bedrock.BedrockModelName) contains a list of available Bedrock models, including models from Anthropic, Amazon, Cohere, Meta, and Mistral.
### Environment variables
You can set your AWS credentials as environment variables:
```
exportAWS_ACCESS_KEY_ID='your-access-key'
exportAWS_SECRET_ACCESS_KEY='your-secret-key'
exportAWS_REGION='us-east-1'# or your preferred region

```

You can then use [`BedrockConverseModel`](https://ai.pydantic.dev/api/models/bedrock/#pydantic_ai.models.bedrock.BedrockConverseModel) by name:
bedrock_model_by_name.py```
frompydantic_aiimport Agent
agent = Agent('bedrock:anthropic.claude-3-sonnet-20240229-v1:0')
...

```

Or initialize the model directly with just the model name:
bedrock_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.bedrockimport BedrockConverseModel
model = BedrockConverseModel('anthropic.claude-3-sonnet-20240229-v1:0')
agent = Agent(model)
...

```

### `provider` argument
You can provide a custom [`BedrockProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.bedrock.BedrockProvider) via the [`provider` argument](https://ai.pydantic.dev/api/models/bedrock/#pydantic_ai.models.bedrock.BedrockConverseModel.__init__). This is useful when you want to specify credentials directly or use a custom boto3 client:
bedrock_model_provider.py```
frompydantic_aiimport Agent
frompydantic_ai.models.bedrockimport BedrockConverseModel
frompydantic_ai.providers.bedrockimport BedrockProvider
# Using AWS credentials directly
model = BedrockConverseModel(
  'anthropic.claude-3-sonnet-20240229-v1:0',
  provider=BedrockProvider(
    region_name='us-east-1',
    aws_access_key_id='your-access-key',
    aws_secret_access_key='your-secret-key',
  ),
)
agent = Agent(model)
...

```

You can also pass a pre-configured boto3 client:
bedrock_model_boto3.py```
importboto3
frompydantic_aiimport Agent
frompydantic_ai.models.bedrockimport BedrockConverseModel
frompydantic_ai.providers.bedrockimport BedrockProvider
# Using a pre-configured boto3 client
bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')
model = BedrockConverseModel(
  'anthropic.claude-3-sonnet-20240229-v1:0',
  provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(model)
...

```

## OpenAI-compatible Models
Many of the models are compatible with OpenAI API, and thus can be used with [`OpenAIModel`](https://ai.pydantic.dev/api/models/openai/#pydantic_ai.models.openai.OpenAIModel) in PydanticAI. Before getting started, check the [OpenAI](https://ai.pydantic.dev/models/#openai) section for installation and configuration instructions.
To use another OpenAI-compatible API, you can make use of the [`base_url`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.openai.OpenAIProvider.__init__) and [`api_key`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.openai.OpenAIProvider.__init__) arguments from `OpenAIProvider`:
deepseek_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.openaiimport OpenAIProvider
model = OpenAIModel(
  'model_name',
  provider=OpenAIProvider(
    base_url='https://<openai-compatible-api-endpoint>.com', api_key='your-api-key'
  ),
)
agent = Agent(model)
...

```

You can also use the `provider` argument with a custom provider class like the [`DeepSeekProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.deepseek.DeepSeekProvider):
deepseek_model_init_provider_class.py```
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.deepseekimport DeepSeekProvider
model = OpenAIModel(
  'deepseek-chat',
  provider=DeepSeekProvider(api_key='your-deepseek-api-key'),
)
agent = Agent(model)
...

```

You can also customize any provider with a custom `http_client`: 
deepseek_model_init_provider_custom.py```
fromhttpximport AsyncClient
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.deepseekimport DeepSeekProvider
custom_http_client = AsyncClient(timeout=30)
model = OpenAIModel(
  'deepseek-chat',
  provider=DeepSeekProvider(
    api_key='your-deepseek-api-key', http_client=custom_http_client
  ),
)
agent = Agent(model)
...

```

### Ollama
To use [Ollama](https://ollama.com/), you must first download the Ollama client, and then download a model using the [Ollama model library](https://ollama.com/library).
You must also ensure the Ollama server is running when trying to make requests to it. For more information, please see the [Ollama documentation](https://github.com/ollama/ollama/tree/main/docs).
#### Example local usage
With `ollama` installed, you can run the server with the model you want to use:
terminal-run-ollama```
ollamarunllama3.2

```

(this will pull the `llama3.2` model if you don't already have it downloaded)
Then run your code, here's a minimal example:
ollama_example.py```
frompydanticimport BaseModel
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.openaiimport OpenAIProvider

classCityLocation(BaseModel):
  city: str
  country: str

ollama_model = OpenAIModel(
  model_name='llama3.2', provider=OpenAIProvider(base_url='http://localhost:11434/v1')
)
agent = Agent(ollama_model, result_type=CityLocation)
result = agent.run_sync('Where were the olympics held in 2012?')
print(result.data)
#> city='London' country='United Kingdom'
print(result.usage())
"""
Usage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65, details=None)
"""

```

#### Example using a remote server
ollama_example_with_remote_server.py```
frompydanticimport BaseModel
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.openaiimport OpenAIProvider
ollama_model = OpenAIModel(
  model_name='qwen2.5-coder:7b', 
The name of the model running on the remote server
[](https://ai.pydantic.dev/models/#__code_58_annotation_1)
  provider=OpenAIProvider(base_url='http://192.168.1.74:11434/v1'), 
The url of the remote server
[](https://ai.pydantic.dev/models/#__code_58_annotation_2)
)

classCityLocation(BaseModel):
  city: str
  country: str

agent = Agent(model=ollama_model, result_type=CityLocation)
result = agent.run_sync('Where were the olympics held in 2012?')
print(result.data)
#> city='London' country='United Kingdom'
print(result.usage())
"""
Usage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65, details=None)
"""

```

### Azure AI Foundry
If you want to use [Azure AI Foundry](https://ai.azure.com/) as your provider, you can do so by using the [`AzureProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.azure.AzureProvider) class.
azure_provider_example.py```
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.azureimport AzureProvider
model = OpenAIModel(
  'gpt-4o',
  provider=AzureProvider(
    azure_endpoint='your-azure-endpoint',
    api_version='your-api-version',
    api_key='your-api-key',
  ),
)
agent = Agent(model)
...

```

### OpenRouter
To use [OpenRouter](https://openrouter.ai), first create an API key at [openrouter.ai/keys](https://openrouter.ai/keys).
Once you have the API key, you can use it with the [`OpenAIProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.openai.OpenAIProvider):
openrouter_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.openaiimport OpenAIProvider
model = OpenAIModel(
  'anthropic/claude-3.5-sonnet',
  provider=OpenAIProvider(
    base_url='https://openrouter.ai/api/v1',
    api_key='your-openrouter-api-key',
  ),
)
agent = Agent(model)
...

```

### Grok (xAI)
Go to [xAI API Console](https://console.x.ai/) and create an API key. Once you have the API key, you can use it with the [`OpenAIProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.openai.OpenAIProvider):
grok_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.openaiimport OpenAIProvider
model = OpenAIModel(
  'grok-2-1212',
  provider=OpenAIProvider(base_url='https://api.x.ai/v1', api_key='your-xai-api-key'),
)
agent = Agent(model)
...

```

### Perplexity
Follow the Perplexity [getting started](https://docs.perplexity.ai/guides/getting-started) guide to create an API key. Then, you can query the Perplexity API with the following:
perplexity_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.openaiimport OpenAIProvider
model = OpenAIModel(
  'sonar-pro',
  provider=OpenAIProvider(
    base_url='https://api.perplexity.ai',
    api_key='your-perplexity-api-key',
  ),
)
agent = Agent(model)
...

```

### Fireworks AI
Go to [Fireworks.AI](https://fireworks.ai/) and create an API key in your account settings. Once you have the API key, you can use it with the [`OpenAIProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.openai.OpenAIProvider):
fireworks_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.openaiimport OpenAIProvider
model = OpenAIModel(
  'accounts/fireworks/models/qwq-32b', # model library available at https://fireworks.ai/models
  provider=OpenAIProvider(
    base_url='https://api.fireworks.ai/inference/v1',
    api_key='your-fireworks-api-key',
  ),
)
agent = Agent(model)
...

```

### Together AI
Go to [Together.ai](https://www.together.ai/) and create an API key in your account settings. Once you have the API key, you can use it with the [`OpenAIProvider`](https://ai.pydantic.dev/api/providers/#pydantic_ai.providers.openai.OpenAIProvider):
together_model_init.py```
frompydantic_aiimport Agent
frompydantic_ai.models.openaiimport OpenAIModel
frompydantic_ai.providers.openaiimport OpenAIProvider
model = OpenAIModel(
  'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free', # model library available at https://www.together.ai/models
  provider=OpenAIProvider(
    base_url='https://api.together.xyz/v1',
    api_key='your-together-api-key',
  ),
)
agent = Agent(model)
...

```

## Implementing Custom Models
To implement support for models not already supported, you will need to subclass the [`Model`](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.Model) abstract base class.
For streaming, you'll also need to implement the following abstract base class:
  * [`StreamedResponse`](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.StreamedResponse)


The best place to start is to review the source code for existing implementations, e.g. [`OpenAIModel`](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py).
For details on when we'll accept contributions adding new models to PydanticAI, see the [contributing guidelines](https://ai.pydantic.dev/contributing/#new-model-rules).
## Fallback
You can use [`FallbackModel`](https://ai.pydantic.dev/api/models/fallback/#pydantic_ai.models.fallback.FallbackModel) to attempt multiple models in sequence until one returns a successful result. Under the hood, PydanticAI automatically switches from one model to the next if the current model returns a 4xx or 5xx status code.
In the following example, the agent first makes a request to the OpenAI model (which fails due to an invalid API key), and then falls back to the Anthropic model.
fallback_model.py```
frompydantic_aiimport Agent
frompydantic_ai.models.anthropicimport AnthropicModel
frompydantic_ai.models.fallbackimport FallbackModel
frompydantic_ai.models.openaiimport OpenAIModel
openai_model = OpenAIModel('gpt-4o', api_key='not-valid')
anthropic_model = AnthropicModel('claude-3-5-sonnet-latest')
fallback_model = FallbackModel(openai_model, anthropic_model)
agent = Agent(fallback_model)
response = agent.run_sync('What is the capital of France?')
print(response.data)
#> Paris
print(response.all_messages())
"""
[
  ModelRequest(
    parts=[
      UserPromptPart(
        content='What is the capital of France?',
        timestamp=datetime.datetime(...),
        part_kind='user-prompt',
      )
    ],
    kind='request',
  ),
  ModelResponse(
    parts=[TextPart(content='Paris', part_kind='text')],
    model_name='claude-3-5-sonnet-latest',
    timestamp=datetime.datetime(...),
    kind='response',
  ),
]
"""

```

The `ModelResponse` message above indicates in the `model_name` field that the result was returned by the Anthropic model, which is the second model specified in the `FallbackModel`.
Note
Each model's options should be configured individually. For example, `base_url`, `api_key`, and custom clients should be set on each model itself, not on the `FallbackModel`.
In this next example, we demonstrate the exception-handling capabilities of `FallbackModel`. If all models fail, a [`FallbackExceptionGroup`](https://ai.pydantic.dev/api/exceptions/#pydantic_ai.exceptions.FallbackExceptionGroup) is raised, which contains all the exceptions encountered during the `run` execution.
[Python >=3.11](https://ai.pydantic.dev/models/#__tabbed_8_1)[Python <3.11](https://ai.pydantic.dev/models/#__tabbed_8_2)
fallback_model_failure.py```
frompydantic_aiimport Agent
frompydantic_ai.exceptionsimport ModelHTTPError
frompydantic_ai.models.anthropicimport AnthropicModel
frompydantic_ai.models.fallbackimport FallbackModel
frompydantic_ai.models.openaiimport OpenAIModel
openai_model = OpenAIModel('gpt-4o', api_key='not-valid')
anthropic_model = AnthropicModel('claude-3-5-sonnet-latest', api_key='not-valid')
fallback_model = FallbackModel(openai_model, anthropic_model)
agent = Agent(fallback_model)
try:
  response = agent.run_sync('What is the capital of France?')
except* ModelHTTPError as exc_group:
  for exc in exc_group.exceptions:
    print(exc)

```

Since [`except*`](https://docs.python.org/3/reference/compound_stmts.html#except-star) is only supported in Python 3.11+, we use the [`exceptiongroup`](https://github.com/agronholm/exceptiongroup) backport package for earlier Python versions:
fallback_model_failure.py```
fromexceptiongroupimport catch
frompydantic_aiimport Agent
frompydantic_ai.exceptionsimport ModelHTTPError
frompydantic_ai.models.anthropicimport AnthropicModel
frompydantic_ai.models.fallbackimport FallbackModel
frompydantic_ai.models.openaiimport OpenAIModel

defmodel_status_error_handler(exc_group: BaseExceptionGroup) -> None:
  for exc in exc_group.exceptions:
    print(exc)

openai_model = OpenAIModel('gpt-4o', api_key='not-valid')
anthropic_model = AnthropicModel('claude-3-5-sonnet-latest', api_key='not-valid')
fallback_model = FallbackModel(openai_model, anthropic_model)
agent = Agent(fallback_model)
with catch({ModelHTTPError: model_status_error_handler}):
  response = agent.run_sync('What is the capital of France?')

```

By default, the `FallbackModel` only moves on to the next model if the current model raises a [`ModelHTTPError`](https://ai.pydantic.dev/api/exceptions/#pydantic_ai.exceptions.ModelHTTPError). You can customize this behavior by passing a custom `fallback_on` argument to the `FallbackModel` constructor.
© Pydantic Services Inc. 2024 to present 
